import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import joblib
from datetime import datetime
import re

def extract_features_for_training(text):
    """VersÃ£o simplificada para treinamento que retorna sÃ³ features"""
    
    # Termos OBRIGATÃ“RIOS para documentos de terra/propriedade rural
    land_property_terms = [
        'escritura', 'propriedade rural', 'fazenda', 'terreno', 'imÃ³vel rural',
        'matrÃ­cula', 'registro de imÃ³vel', 'cartÃ³rio de registro', 'Ã¡rea rural'
    ]
    
    # Termos OBRIGATÃ“RIOS para CDA/ArmazÃ©m
    cda_terms = [
        'certificado de depÃ³sito agropecuÃ¡rio', 'cda', 'armazÃ©m', 'depositÃ¡rio', 
        'warrant agropecuÃ¡rio', 'wa', 'produto agropecuÃ¡rio', 'safra'
    ]
    
    # Termos tÃ©cnicos de agricultura
    agro_technical_terms = [
        'hectares', 'toneladas', 'sacas', 'soja', 'milho', 'trigo', 'algodÃ£o',
        'bovinos', 'suÃ­nos', 'aves', 'cultivo', 'plantio', 'colheita'
    ]
    
    # Termos de documentaÃ§Ã£o oficial
    official_doc_terms = [
        'cartÃ³rio', 'tabeliÃ£o', 'oficial pÃºblico', 'reconhecimento de firma',
        'protocolo', 'livro de registro', 'certidÃ£o', 'autenticaÃ§Ã£o'
    ]
    
    # Termos que INVALIDAM completamente o documento
    invalid_terms = [
        'currÃ­culo', 'cv', 'curriculum', 'experiÃªncia profissional', 'formaÃ§Ã£o acadÃªmica',
        'habilidades', 'skills', 'trabalhou', 'emprego', 'empresa', 'cargo',
        'universidade', 'faculdade', 'graduaÃ§Ã£o', 'pÃ³s-graduaÃ§Ã£o', 'mestrado',
        'doutorado', 'curso', 'disciplina', 'professor', 'aluno', 'estudante'
    ]
    
    text_lower = text.lower()
    
    # Contagens
    land_count = sum(1 for term in land_property_terms if term in text_lower)
    cda_count = sum(1 for term in cda_terms if term in text_lower)
    agro_tech_count = sum(1 for term in agro_technical_terms if term in text_lower)
    official_count = sum(1 for term in official_doc_terms if term in text_lower)
    invalid_count = sum(1 for term in invalid_terms if term in text_lower)
    
    # VerificaÃ§Ãµes
    has_dates = len(re.findall(r'\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}', text))
    has_cpf_cnpj = len(re.findall(r'\d{3}\.\d{3}\.\d{3}-\d{2}|\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}', text))
    has_money = len(re.findall(r'R\$\s*[\d.,]+|reais|valor|preÃ§o', text))
    has_area_measures = len(re.findall(r'\d+[\s]*(?:hectares?|ha|mÂ²|metros?|alqueires?)', text_lower))
    has_registry_numbers = len(re.findall(r'(?:matrÃ­cula|registro|protocolo)[\s\n]*n?[ÂºoÂ°]?\s*[\d\-\.]+', text_lower))
    
    text_length = len(text)
    word_count = len(text.split())
    
    # Features para o modelo ML
    features = [
        land_count + cda_count,      # 0: Documentos vÃ¡lidos
        agro_tech_count,             # 1: Termos tÃ©cnicos agro
        invalid_count * -10,         # 2: Penalidade por termos invÃ¡lidos
        has_dates,                   # 3: Datas
        has_cpf_cnpj + has_registry_numbers,  # 4: IdentificaÃ§Ãµes
        has_money,                   # 5: Valores
        has_area_measures,           # 6: Medidas de Ã¡rea
        min(text_length/1000, 10),   # 7: Tamanho normalizado
        min(word_count/100, 10),     # 8: Palavras normalizadas
    ]
    
    return features

class DocumentValidatorTrainer:
    def __init__(self):
        self.models = {
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),
            'svm': SVC(probability=True, random_state=42)
        }
        self.best_model = None
        self.scaler = StandardScaler()
        
    def load_data(self, csv_path='data/synthetic_data.csv'):
        """Carrega dados do CSV"""
        print(f"ğŸ“‚ Carregando dados de: {csv_path}")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Arquivo nÃ£o encontrado: {csv_path}")
        
        df = pd.read_csv(csv_path)
        print(f"âœ… Dados carregados: {len(df)} amostras")
        
        return df
    
    def prepare_features(self, df):
        """Prepara features para treinamento"""
        print("ğŸ”§ Preparando features...")
        
        # Selecionar apenas colunas numÃ©ricas para features
        feature_columns = [
            'legal_terms_count', 'has_dates', 'text_length', 
            'number_count', 'special_chars_count', 'legal_density',
            'keywords_present', 'has_coordinates', 'uppercase_ratio'
        ]
        
        # Verificar se todas as colunas existem
        missing_cols = [col for col in feature_columns if col not in df.columns]
        if missing_cols:
            print(f"âš ï¸  Colunas faltando: {missing_cols}")
            # Criar colunas faltando com valores padrÃ£o
            for col in missing_cols:
                df[col] = 0
        
        X = df[feature_columns].fillna(0)
        y = df['is_valid'].astype(int)
        
        print(f"ğŸ“Š Features shape: {X.shape}")
        print(f"ğŸ¯ Target distribution: {y.value_counts().to_dict()}")
        
        return X, y, feature_columns
    
    def train_models(self, X, y):
        """Treina mÃºltiplos modelos e seleciona o melhor"""
        print("ğŸš€ Iniciando treinamento dos modelos...")
        
        # Dividir dados
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"ğŸ“ˆ Dados de treino: {X_train.shape[0]} amostras")
        print(f"ğŸ“Š Dados de teste: {X_test.shape[0]} amostras")
        
        best_score = 0
        results = {}
        
        for name, model in self.models.items():
            print(f"\nğŸ”„ Treinando {name}...")
            
            # Criar pipeline com normalizaÃ§Ã£o
            pipeline = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', model)
            ])
            
            # Treinar modelo
            pipeline.fit(X_train, y_train)
            
            # Avaliar
            y_pred = pipeline.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            results[name] = {
                'model': pipeline,
                'accuracy': accuracy,
                'predictions': y_pred,
                'y_test': y_test
            }
            
            print(f"âœ… {name} - AcurÃ¡cia: {accuracy:.4f}")
            
            # Salvar melhor modelo
            if accuracy > best_score:
                best_score = accuracy
                self.best_model = pipeline
                self.best_model_name = name
        
        print(f"\nğŸ† Melhor modelo: {self.best_model_name} (AcurÃ¡cia: {best_score:.4f})")
        
        return results, X_test, y_test
    
    def evaluate_model(self, results, X_test, y_test):
        """Avalia o melhor modelo em detalhes"""
        print(f"\nğŸ“‹ AvaliaÃ§Ã£o detalhada do melhor modelo ({self.best_model_name}):")
        
        best_result = results[self.best_model_name]
        y_pred = best_result['predictions']
        
        # RelatÃ³rio de classificaÃ§Ã£o
        print("\nğŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o:")
        print(classification_report(y_test, y_pred, 
                                  target_names=['InvÃ¡lido', 'VÃ¡lido']))
        
        # Matriz de confusÃ£o
        print("\nğŸ” Matriz de ConfusÃ£o:")
        cm = confusion_matrix(y_test, y_pred)
        print(cm)
        
        # ImportÃ¢ncia das features (se disponÃ­vel)
        if hasattr(self.best_model.named_steps['classifier'], 'feature_importances_'):
            print("\nğŸ¯ ImportÃ¢ncia das Features:")
            importances = self.best_model.named_steps['classifier'].feature_importances_
            feature_names = [
                'legal_terms_count', 'has_dates', 'text_length', 
                'number_count', 'special_chars_count', 'legal_density',
                'keywords_present', 'has_coordinates', 'uppercase_ratio'
            ]
            
            for name, importance in zip(feature_names, importances):
                print(f"  {name}: {importance:.4f}")
    
    def save_model(self, model_dir='models'):
        """Salva o melhor modelo treinado"""
        os.makedirs(model_dir, exist_ok=True)
        
        model_path = os.path.join(model_dir, 'document_validator.pkl')
        
        # Salvar modelo
        joblib.dump(self.best_model, model_path)
        
        # Salvar metadados
        metadata = {
            'model_type': self.best_model_name,
            'trained_at': datetime.now().isoformat(),
            'feature_names': [
                'legal_terms_count', 'has_dates', 'text_length', 
                'number_count', 'special_chars_count', 'legal_density',
                'keywords_present', 'has_coordinates', 'uppercase_ratio'
            ]
        }
        
        metadata_path = os.path.join(model_dir, 'model_metadata.json')
        import json
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"ğŸ’¾ Modelo salvo em: {model_path}")
        print(f"ğŸ“‹ Metadados salvos em: {metadata_path}")
        
        return model_path
    
    def predict(self, features):
        """Faz prediÃ§Ã£o com o modelo treinado"""
        if self.best_model is None:
            raise ValueError("Modelo nÃ£o foi treinado ainda!")
        
        # Converter para formato correto se necessÃ¡rio
        if isinstance(features, dict):
            feature_order = [
                'legal_terms_count', 'has_dates', 'text_length', 
                'number_count', 'special_chars_count', 'legal_density',
                'keywords_present', 'has_coordinates', 'uppercase_ratio'
            ]
            features = [[features.get(f, 0) for f in feature_order]]
        
        prediction = self.best_model.predict(features)[0]
        probability = self.best_model.predict_proba(features)[0]
        
        return {
            'is_valid': bool(prediction),
            'confidence': float(max(probability)),
            'probabilities': {
                'invalid': float(probability[0]),
                'valid': float(probability[1])
            }
        }

def main():
    print("ğŸ¤– Iniciando treinamento do modelo de validaÃ§Ã£o de documentos...")
    
    trainer = DocumentValidatorTrainer()
    
    try:
        # Carregar dados
        df = trainer.load_data()
        
        # Preparar features
        X, y, feature_columns = trainer.prepare_features(df)
        
        # Treinar modelos
        results, X_test, y_test = trainer.train_models(X, y)
        
        # Avaliar melhor modelo
        trainer.evaluate_model(results, X_test, y_test)
        
        # Salvar modelo
        model_path = trainer.save_model()
        
        print("\nâœ… Treinamento concluÃ­do com sucesso!")
        print(f"ğŸ¯ Modelo final: {trainer.best_model_name}")
        print(f"ğŸ’¾ Arquivo do modelo: {model_path}")
        
        # Teste rÃ¡pido
        print("\nğŸ§ª Teste rÃ¡pido do modelo:")
        test_features = {
            'legal_terms_count': 5,
            'has_dates': 1,
            'text_length': 500,
            'number_count': 10,
            'special_chars_count': 0,
            'legal_density': 0.1,
            'keywords_present': 3,
            'has_coordinates': 1,
            'uppercase_ratio': 0.15
        }
        
        prediction = trainer.predict(test_features)
        print(f"Resultado: {prediction}")
        
    except Exception as e:
        print(f"âŒ Erro durante o treinamento: {str(e)}")
        raise

if __name__ == "__main__":
    main()